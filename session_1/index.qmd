---
title: "Solving DSGE models"
format:
    revealjs:
        navigation-mode: grid
        layout: wide
        toc: true
        toc-depth: 1
        toc-title: Plan
---

## Plan for the rest of the course

- session 4: 
    - DSGE: solution algorithms
- session 5:
    - open economy
- session 6:
    - monetary policy
- session 7:
    - heterogeneity and DSGE models

# Introduction

## 

::: columns

:::: {.column width=70%}

What is the main specificity of economic modeling?

::: {.fragment}

In (macro)economics, we *model* the behaviour of economic agents by specifying:

- their objective $\max_{c_t} \beta^t U(c_t)$, $\max \pi_t$, ...
- their constraints (budget constraint, econ. environment...)

:::

::::

:::: {.column width=30%}

![](assets/lucifer.jpg){.fragment}



::::
:::


::: {.fragment}

This has important implications:

- macro models are *forward looking*
- macro models need to be __solved__

:::


## Dynare

::: columns

:::: column

- ~2000 Michel Juillard created a specialized software to solve DSGE models
    - DSGE: Dynamic Stochastic General Equilibrium

:::: {.fragment}

- It has been widely adopted:
    - early version in Gauss
    - then Matlab/Octave/Scilab
    - latest version in Julia

::::


::::

:::: column

![Michel Juillard](assets/michel.jpeg)

::::

:::

## DSGE Models in institutions

- Nowadays most DSGE models built in institutions have a Dynare version (IMF/GIMF, EC/Quest, ECB/, NYFed/FRBNY)
    - they are usually based on the *midsize model* from Smets & Wouters (10 equations)
    - but have grown up a lot (>>100 equations)
- Although they are (very slowly) diversifying their model

# Solving a model

## Model

A very concise representation of a model

$$\mathbb{E}_t \left[ f(y_{t+1}, y_t, y_{t-1}, \epsilon_t) \right]= 0$$

::: r-stack

:::: {.fragment .current-visible}
The ingredients:

- $y_t\in\mathbb{R}^n$: the vector of endogenous variables
- $\epsilon_t\in\mathbb{R}^{n_e}$: the vector of exogenous variables
    - we assume that $\epsilon_t$ is zero-meaned gaussian process
- $f: \mathbb{R}^n\rightarrow \mathbb{R}^n$: the model equations

::::
:::: {.fragment .current-visible}

The solution: $g$ such that $$\forall t, y_t = g(y_{t-1},\epsilon_t)$$

::::

:::

::: notes
The situation is different when one is making a perfect foresight simulation.
:::

## The timing of the equations

::: {.callout-tip}

In dynare the model equations are coded in the `model; ... ; end;` block.

:::

New information arrives with the innovations $\epsilon_t$. 

At date $t$, the information set is spanned by $\mathcal{F}_t = \mathcal{F} (\cdots, \epsilon_{t-3}, \epsilon_{t-2}, \epsilon_{t-1}, \epsilon_t)$

By convention *an endogenous variable has a subscript $t$ if it is known first at date $t$*.

## Example

__The timing of equations__

Using Dynare's timing conventions:

- Write the production function in the RBC

- Write the law of motion for capital $k$, with a depreciation rate $\delta$ and investment $i$
    - when is capital known?
    - when is investment known?

- Add a multiplicative investment efficiency shock $\chi_t$. Assume it is an $AR1$ driven by innovation $\eta_t$ and autorcorrelation $\rho_{\chi}$

##  Steady-state

The deterministic steady-state satisfies:

$$ f(\overline{y},\overline{y}, \overline{y}, 0)= 0$$

Often, there is a closed-form solution.



Otherwise, one must resort to a numerical solver to solve $\overline{y}\rightarrow f( f(\overline{y},\overline{y}, \overline{y}, 0))$


::: {.callout-tip collapse="true"}

In dynare the steady-state values are provided in the `steadystate_model; ... ; end;` block. One can check they are correct using the `check;` statement.

To find numerically the steady-state: `steady;`.

:::


## The implicit system {auto-animate="true"}

Replacing the solution $$y_t = g(y_{t-1},\epsilon_t)$$ in the system
$$\mathbb{E}_t \left[ f(y_{t+1}, y_t, y_{t-1}, \epsilon_t) \right]= 0$$

we obtain:

$$\mathbb{E}_t \left[ f(g(g(y_{t-1},\epsilon_{t}),\epsilon_{t+1}), g(y_{t-1},\epsilon_t), y_{t-1}, \epsilon_t) \right]= 0$$

It is an equation defining implicitly the function $g()$

## The state-space{auto-animate="true"}

$$\mathbb{E}_t \left[ f(g(g(y_t,\epsilon_{t+1}),\epsilon_t), g(y_{t-1},\epsilon_t), y_{t-1}, \epsilon_t) \right]= 0$$

In this expression, $y_{t-1},\epsilon_t$ is the state-space.

. . .

Dropping the time subscripts, the equation must be satisfied for any realization of $(y,\epsilon)$
$$\forall (y,\epsilon)\  \Phi(g)(y,\epsilon) = \mathbb{E}_{\epsilon'} \left[ f(g(g(y,\epsilon),\epsilon'), g(y,\epsilon), y, \epsilon) \right]= 0$$

It is a functional equation $\Phi(g)=0$

## Expected shocks

Assume $|y_t-\overline{y}|<<1$,$|\epsilon|<<1$,$|\epsilon'|<<1$

First we can perform a Taylor expansion with respect to future shock:

\begin{align}
 & & \mathbb{E}_{\epsilon'} \left[ f(g(g(y,\epsilon),\epsilon'), g(y,\epsilon), y, \epsilon) \right]  \\
= & &  \mathbb{E}_{\epsilon'}\left[ f(g(g(y,\epsilon),0), g(y,\epsilon), y, \epsilon) \right] \\ 
  & & +  \mathbb{E}_{\epsilon'} \left[ f^{\prime}_{y_{t+1}}(g(g(y,\epsilon),0), g(y,\epsilon), y, \epsilon) g^{\prime}_{\epsilon} \epsilon^\prime\right] + o(\epsilon^{\prime}) \\
\approx & & f(g(g(y,0),\epsilon), g(y,\epsilon), y, \epsilon)
\end{align}

. . .

This uses the fact that $\mathbb{E}\left[ \epsilon^{\prime}\right] = 0$. At first order, expected shocks play no role.


## First order perturbation

We are left with the system:

$$F(y,\epsilon) = f(g(g(y,0),\epsilon), g(y,\epsilon), y, \epsilon) = 0$$

We can now use a variant of the implicit function theorem to recover a first approximation of $g$ as:
$$g(y,\epsilon) = \overline{y} + g^{\prime}_{y} (y-\overline{y}) + g^{\prime}_e \epsilon_t $$

We can obtain the unknown quantities $g^{\prime}_y$,  and $g^{\prime}_e$ using the methods of undeterminate coefficients.

Use these to write $F^{\prime}_y(\overline{y}, 0) = 0$ and $F^{\prime}_\epsilon(\overline{y}, 0) = 0$.

## The transition matrix

Recall the system:
$$F(y,\epsilon) = f(g(g(y,0),\epsilon), g(y,\epsilon), y, \epsilon) = 0$$


We have $$F^{\prime}_y(\overline{y}, 0) = f^{\prime}_{y_{t+1}} g^{\prime}_y g^{\prime}_y + f^{\prime}_{y_{t}} g^{\prime}_y + f^{\prime}_{y_{t-1}} = 0 $$

Or $$A X^2 + B X + C$$ where $A,B,C$ and $X=g^{\prime}_y$ are square matrices.


## The Riccatti Equation

Recall the system:
$$F(y,\epsilon) = f(g(g(y,0),\epsilon), g(y,\epsilon), y, \epsilon) = 0$$


We have $$F^{\prime}_y(\overline{y}, 0) = f^{\prime}_{y_{t+1}} g^{\prime}_y g^{\prime}_y + f^{\prime}_{y_{t}} g^{\prime}_y + f^{\prime}_{y_{t-1}} = 0 $$

This is a specific Riccatti equation  $$A X^2 + B X + C$$ where $A,B,C$ and $X=g^{\prime}_y$ are square matrices $\in \mathbb{R}^n \times \mathbb{R}^n$

## First Order Deterministic Model

Let's pause a minute to observe the first order deterministic model:
$$A X^2 + B X + C$$

From our intuition in dimension 1, we know there must be multiple solutions

- how do we find them?
- how do we select the right ones?

Obviously, the dynamics of the model are given by $y_t = X y_{t-1}$.

For the solution  to the model to be stationary, the spectral radius of $X$ should be smaller than 1.

::: notes

TODO: make a specific slide about the dynamics

:::

## Multiplicity of solution

It is possible to show that the system is associated with $2 n$ generalized eigenvalues:

$$|\lambda_1| \leq \cdots \leq |\lambda_{2 n}|$$

For each choice $C$ of $n$ eigenvalues ($|C|=n$), a specific fundamental solution $X_C$ can be *constructed*. It has eigenvalues $C$.

This is at least $\left(\begin{matrix} 2 n \\ n \end{matrix}\right)$ different combinations.

Then any linear combination of two solutions is also a solution...

## Well defined model



A model is well defined when there is __exactly one solution that is non divergent__.


This implies: 

$$|\lambda_1| \leq \cdots \leq |\lambda_{n}| \leq 1 < |\lambda_{n+1}| \leq \cdots \leq |\lambda_{2 n}|$$


## Example 1: 

Forward looking inflation:

$$\pi_t = \alpha \pi_{t+1}$$
with $\alpha>1$. 

Is it well defined?

. . .

We can rewrite the system as:
$$\alpha \pi_{t+1} - \pi_t + 0 \pi_{t-1} = \pi_{t+1} - (\frac{1}{\alpha} + 0 )\pi_t + \left(\frac{1}{\alpha} 0\right) \pi_{t-1}$$

The generalized eigenvalues are $0\leq 1 < \frac{1}{\alpha}$.
The unique solution is $\pi_t=0 \pi_{t-1}$


## Example 2: 

Debt accumulation equation by a rational agent:

$$b_{t+1} - (1+\frac{1}{\beta}) b_t + \frac{1}{\beta} b_{t-1} = 0$$

Is it well-defined?

. . .

Two generalized eigenvalues $\lambda_1=1 < \lambda_2=\frac{1}{\beta}$

The unique solution is $b_t = b_{t-1}$.

- it is a *unit-root*: any initial deviation in $b_{t-1}$ has persistent effects


## Example 3: 

Productivity process: 
$$z_t = \rho z_{t-1}$$
with $\rho<1$: [well defined]{.fragment}

. . .

In that case there is a hidden infinite eigenvalue $\infty$ associated to $z_{t+1}$.

The generalized eigenvalues are $\lambda_1 = \rho \leq 1 < \lambda_2 = \infty$

More generally, any variable that does not appear in $t+1$ creates one infinite generalized eigenvalue.

## A criterium for well-definedness

Looking again at the list of eigenvalues we can aside the infinite ones.

The model is well specified iff we can sort the eigenvalues as:

$$|\lambda_1| \leq \cdots \leq |\lambda_{n}| \leq 1 < |\lambda_{n+1}| \leq \cdots  |\lambda_{n+k}| \leq \underbrace{|\lambda_{n+k+1}| \cdots \leq |\lambda_{2 n}|}_{\text{infinite eigenvalues}}$$


::: {.callout-important}

### Blanchard-Kahn criterium

The model satisfies the Blanchard-Kahn criterium if the number of eigenvalues greater than one, is exactly equal to the number of variables *appearing* in $t+1$.

In that case the model is well-defined.

:::

## Computing the solution

There are several classical methods to compute the solution to the algebraic Riccatti equation: $$A X^2+ B X + C=0$$

- qz decomposition
    - traditionnally used in the DSGE literature since Chris Sims
    - a little bit unintuitive
- cyclic reduction
    - new default in dynare, more adequate for big models
- linear time iteration
    - conceptually very simple

## Computing the shocks

Recall:
$$F(y,\epsilon) = f(g(g(y,\epsilon),0), g(y,\epsilon), y, \epsilon) = 0$$


We have $$F^{\prime}_e(\overline{y}, 0) =  f^{\prime}_{y_{t+1}}  g^{\prime}_y  g^{\prime}_e + f^{\prime}_{y_{t}} g^{\prime}_e + f^{\prime}_{\epsilon_t} = 0 $$

Now this is easy:

$$g^{\prime}_e = - (f^{\prime}_{y_{t+1}}  g^{\prime}_y + f^{\prime}_{y_{t}} )^{-1}  f^{\prime}_{\epsilon_t} = 0 $$


## The model solution

The result of the model solution:
$$y_t = g_y y_{t-1} + g_e \epsilon_t$$

It is an AR1, driven by exogenous shock $\epsilon_t$.

Because it is a well known structure, one can compute

- impulse response functions
- stochastic simulations
- implied moments

Also, one can easily compute the likelihood of the model given some data to perform model estimation.

## Linear Time Iteration (1)

Recall the system to solve:
$$F(y,\epsilon) = f(g(g(y,\epsilon),0), g(y,\epsilon), y, \epsilon) = 0$$

bnut now assume the decision rules today and tomorrow are different:

- today: $y_t = g(y_{t-1}, \epsilon_t) = \overline{y} + X y_{t-1} + g_y \epsilon_t$
- tomorrow: $y_{t+1} = \tilde{g}(y_t, \epsilon_{t+1}) + \tilde{X} y_{t-1} + \tilde{g}_y \epsilon_t$

Then the Ricatti equation is written:

$$A \tilde{X} X + B X + C = 0$$


## Linear Time Iteration (2)

The linear time iteration algorithm consists in solving the decision rule $X$  today as a function of decision rule tomorrow $\tilde{X}$.

This corresponds to the simple formula:

$$X = -(A\tilde{X} + B)^{-1} C$$

And the full algorithm can be described as:

- choose $X_0$
- for any $X_n$, compute $X_{n+1} = T(X_n) = -(A X_n + B)^{-1} C$
    - repeat until convergence

## Linear Time Iteration (3)

It can be shown that, started from a random initial guess, the linear time-iteration algorithm always converges to the solution $X$ with the smallest modulus:

$$\underbrace{|\lambda_1| \leq \cdots  \leq |\lambda_n|}_{\text{Selected eigenvalues}} \leq |\lambda_{n+1}|\cdots \leq |\lambda_{2n}|$$

In other words, it finds the right solution when the model is well specified.

How do you check it is well specified?

- $\lambda_n$ is the biggest eigenvalue of solution $X$
- if only we could measure $\lambda_{n+1}$...

## Linear Time Iteration (4)

One can show that:

- $\lambda_{n}$ is the spectral radius of the simulation operator $X: \mathbb{R}^n \rightarrow \mathbb{R}^n$
- $\frac{1}{\lambda_{n+1}}$ is the spectral radius of the derivative of the time-iteration operator $T^{\prime}: \mathbb{R}^n \times \mathbb{R}^n \rightarrow \mathbb{R}^n \times \mathbb{R}^n$

If both $\rho(X)\leq 1$ and $\rho(T^{\prime})<1$, the model is well specified.

This approach highlights the fact that for a model to be well defined, state dynamics and iterated expectations must be stable.

## Example

Take a generic first order model
$$b_{t+1} - (\lambda_1 + \lambda_2) b_t + \lambda_1 \lambda_2 b_{t-1} = 0$$

write decision rule today as $b_{t} = \lambda_{n} b_{t-1}$ and decision rule tomorrow as $b_{t+1} = \lambda_{n-1} b_t$.

- What is the function $\lambda_n=f(\lambda_{n-1})$?
- Can you prove that $\lambda_n$ always converges to a fixed point $\overline{\lambda}$?
- Compute $f^{\prime}(\overline{\lambda})$
- Prove that knowing $\lambda$ and $f^{\prime}(\lambda)$ is sufficient to know that the model satisfies Blanchard-Kahn conditions.


# Conclusion

## What can you do with the solution

- The solution of a model has an especially simple form: it is an AR1
    - $y_t = X y_{t-1} + Y \epsilon_t$
    - where the covariances $\Sigma$ of $\epsilon_t$ can be chosen by the modeler

::: r-stack

:::: {.fragment .current-visible}

- On can thus:
    - compute (conditional and unconditional) moments
    - perform impulse response function, that is compute the effect of a shock

::::
:::: {.fragment .current-visible}

- Going further:
    - "estimate" the model: compute the likelihood of a solution and maximize it by choosing the right parameters
    - "identify" shocks in the data

::::
:::

## Other Dynare features

Dynare allows to perform relatively easily

- higher order approximation
- model estimation 
- ramsey plan
- discretionary policy
- ...

. . .

You should not really be using all these features unless you know what you are doing...